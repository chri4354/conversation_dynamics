{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "prol = sentences[0]\n",
    "proc = sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92300\n",
      "num sentences total:  92300 using 50000 words\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "\n",
    "# num_words we use\n",
    "num_words = 50000\n",
    "\n",
    "# if we want two-sentence inputs\n",
    "prol = [str(prol[k:k+1][0])+str(prol[k+1:k+2]) for k in range(len(prol))]\n",
    "proc = [str(proc[k:k+1][0])+str(proc[k+1:k+2]) for k in range(len(proc))]\n",
    "\n",
    "X = prol + proc\n",
    "print(len(X))\n",
    "print(\"num sentences total: \", len(X), \"using {} words\".format(num_words))\n",
    "\n",
    "\n",
    "# prolife is zero prochoice is 1\n",
    "Y = np.hstack((np.zeros(shape=len(prol)), np.ones(shape=len(proc))))\n",
    "\n",
    "# split data into train and test set, and shuffle it\n",
    "idx_train, idx_test, y_train, y_test = train_test_split(\n",
    "                np.array([range(len(Y))]).T, Y, test_size=0.2, random_state=0)\n",
    "idx_train = np.array(idx_train.T[0],dtype='int')\n",
    "idx_test = idx_test.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I actually think your draft argument is the strongest one for a libertarian[\" You are seeing it as analogous to the violation of the mother\\'s bodily autonomy but I actually think it\\'s a bit stronger when used as analogous to the violation of the child\\'s, as in both cases it\\'s the smaller, weaker party (one citizen vs govt) being sacrificed for the ostensible good of the larger organism\"]'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes words to numbers\n",
    "# tokenizing based on train data (i.e. will see new words in testset)\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=num_words, lower=True,\n",
    "                                                filters='!\"\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ‚Äù][',\n",
    "                                               split=\" \",oov_token=0)\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts([X[k] for k in idx_train])\n",
    "X2 = np.array(tokenizer.texts_to_sequences([X[k] for k in range(len(X))]))\n",
    "x_train = X2[idx_train]\n",
    "x_test = X2[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I actually think your draft argument is the strongest one for a libertarian[\" You are seeing it as analogous to the violation of the mother's bodily autonomy but I actually think it's a bit stronger when used as analogous to the violation of the child's, as in both cases it's the smaller, weaker party (one citizen vs govt) being sacrificed for the ostensible good of the larger organism\"]\n",
      "[9, 139, 48, 35, 3579, 111, 6, 1, 4966, 59, 14, 3, 3224, 10, 15, 1099, 11, 22, 2990, 2, 1, 1471, 5, 1, 725, 206, 203, 23, 9, 139, 48, 46, 3, 569, 3084, 60, 260, 22, 2990, 2, 1, 1471, 5, 1, 1487, 22, 12, 193, 327, 46, 1, 3134, 7869, 577, 59, 4043, 918, 9293, 56, 6016, 14, 1, 27370, 131, 5, 1, 2246, 356]\n"
     ]
    }
   ],
   "source": [
    "print(X[90])\n",
    "print(X2[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24382 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# embeddings_index = {}\n",
    "# maxlen = 100\n",
    "# EMBEDDING_DIM = 100\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# import os\n",
    "# f = open(os.path.join('','glove.6B.100d.txt'))\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,CuDNNLSTM\n",
    "\n",
    "# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# embedding_layer = Embedding(len(word_index) + 1,\n",
    "#                             EMBEDDING_DIM,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=maxlen,\n",
    "#                             trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfM9CevJVGlwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120ab5400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist([len(x) for x in X2],  bins=50,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "73838 train sequences\n",
      "18460 test sequences\n",
      "Pad sequences (samples x time)\n",
      "removing len zero and len1 sentences...\n",
      "73838\n",
      "18460\n",
      "x_train shape: (73838, 100)\n",
      "x_test shape: (18460, 100)\n",
      "Train...\n",
      "Train on 73838 samples, validate on 18460 samples\n",
      "Epoch 1/10\n",
      "73838/73838 [==============================] - 270s 4ms/step - loss: 0.6395 - acc: 0.6258 - val_loss: 0.5911 - val_acc: 0.6766\n",
      "Epoch 2/10\n",
      "73838/73838 [==============================] - 295s 4ms/step - loss: 0.5165 - acc: 0.7410 - val_loss: 0.5749 - val_acc: 0.7054\n",
      "Epoch 3/10\n",
      "60256/73838 [=======================>......] - ETA: 45s - loss: 0.4311 - acc: 0.7907 ETA: 46s - loss: 0.43"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM,GRU, Bidirectional,CuDNNLSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "maxlen = 100\n",
    "print('Loading data...')\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print('removing len zero and len1 sentences...')\n",
    "\n",
    "nonzero1 = x_train.sum(1)>1\n",
    "x_train = x_train[nonzero1]\n",
    "y_train = y_train[nonzero1]\n",
    "\n",
    "print(len(x_train))\n",
    "nonzero2 = x_test.sum(1)>1\n",
    "x_test = x_test[nonzero2]\n",
    "y_test = y_test[nonzero2]\n",
    "print(len(x_test))\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 50, input_length=maxlen))\n",
    "model.add(Bidirectional(GRU(20)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=[x_test, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "[[0.89091283]]\n"
     ]
    }
   ],
   "source": [
    "example= ['I believe women should be able to choose their own fate they should not be restrained']\n",
    "# prolife is zero prochoice is 1\n",
    "ex = tokenizer.texts_to_sequences(example)\n",
    "ex = np.array(sequence.pad_sequences(ex, maxlen=maxlen))\n",
    "print(ex.shape)\n",
    "print(model.predict(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3688292]]\n"
     ]
    }
   ],
   "source": [
    "example = ['abortion is wrong It should be made illegal']\n",
    "ex = tokenizer.texts_to_sequences(example)\n",
    "ex = np.array(sequence.pad_sequences(ex, maxlen=maxlen))\n",
    "print(model.predict(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('bilstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # loading\n",
    "# with open('tokenizer.pickle', 'rb') as handle:\n",
    "#     tokenizer = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
