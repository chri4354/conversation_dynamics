{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM with synonym augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('data_files/abortion/sentences2.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "prol = sentences[0]\n",
    "proc = sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data augmentation via synonym substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def synonimize(word, pos=None):\n",
    "    \"\"\" Get synonyms of the word / lemma \"\"\" \n",
    "    try:\n",
    "        # map part of speech tags to wordnet\n",
    "        pos = {'NN': wn.NOUN,'JJ':wn.ADJ,'VB':wn.VERB,'RB':wn.ADV}[pos[:2]]\n",
    "    except:\n",
    "        # or just return the original word\n",
    "#       print(\"OUCH {} {}\".format(word, pos))\n",
    "        return [word]\n",
    "\n",
    "    synsets = wn.synsets(word, pos)\n",
    "    synonyms = []\n",
    "    for synset in synsets:\n",
    "        for sim in  synset.similar_tos():\n",
    "            synonyms += sim.lemma_names()\n",
    "    \n",
    "    # return list of synonyms or just the original word\n",
    "    return synonyms or [word]\n",
    "\n",
    "def vary_sentence(sentence):\n",
    "    \"\"\" Create variations of a sentence using synonyms \"\"\"\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    words = []\n",
    "    for (word, pos) in pos_tags:\n",
    "        synonyms = synonimize(word, pos)\n",
    "        picked = np.random.choice(synonyms)\n",
    "        words.append(picked)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "\n",
    "# print(vary_sentence(\"I like boring people.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo = False\n",
    "# if redo:\n",
    "#     n_syns=2\n",
    "#     print(len(prol),len(proc))\n",
    "#     for k in range(n_syns):\n",
    "#         prol = [[str(k)]+[str(vary_sentence(k))] for k in prol]\n",
    "#         proc = [[str(k)]+[str(vary_sentence(k))] for k in proc]\n",
    "#         import itertools\n",
    "#         prol = list(itertools.chain(*prol))\n",
    "#         proc = list(itertools.chain(*proc))\n",
    "#         print(len(prol),len(proc))\n",
    "#         with open('data_files/abortion/sentences_syn2.pkl', 'wb') as f:\n",
    "#             pickle.dump([prol,proc], f)\n",
    "\n",
    "# else:\n",
    "#     [prol,proc] = pickle.load('data_files/abortion/sentences_syn2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(prol),len(proc))\n",
    "# with open('data_files/abortion/sentences_syn2.pkl', 'wb') as f:\n",
    "#     pickle.dump([prol,proc], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52968 48124\n"
     ]
    }
   ],
   "source": [
    "# combine two sentences\n",
    "\n",
    "prol = [str(prol[k:k+1][0])+str(prol[k+1:k+2]) for k in range(len(prol))]\n",
    "proc = [str(proc[k:k+1][0])+str(proc[k+1:k+2]) for k in range(len(proc))]\n",
    "print(len(prol),len(proc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # num_words we use\n",
    "# num_words = 50000\n",
    "\n",
    "# # if we want two-sentence input# prolife is zero prochoice is 1\n",
    "\n",
    "# X = prol + proc\n",
    "# print(len(X))\n",
    "# print(\"num sentences total: \", len(X), \"using {} words\".format(num_words))\n",
    "\n",
    "\n",
    "# # prolife is zero prochoice is 1\n",
    "# Y = np.hstack((np.zeros(shape=len(prol)), np.ones(shape=len(proc))))\n",
    "\n",
    "# # split data into train and test set, and shuffle it\n",
    "# idx_train, idx_test, y_train, y_test = train_test_split(\n",
    "#                 np.array([range(len(Y))]).T, Y, test_size=0.2, random_state=0)\n",
    "# idx_train = np.array(idx_train.T[0],dtype='int')\n",
    "# idx_test = idx_test.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuki/anaconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "num_words = 50000\n",
    "\n",
    "# no info leak here\n",
    "X = prol + proc\n",
    "Y = np.hstack((np.zeros(shape=len(prol)), np.ones(shape=len(proc))))\n",
    "\n",
    "# split data into train and test set, and shuffle it\n",
    "idx_train, idx_test, y_train, y_test = train_test_split(\n",
    "                np.array([range(len(Y))]).T, Y, test_size=0.2, random_state=0)\n",
    "idx_train = np.array(idx_train.T[0],dtype='int')\n",
    "idx_test = idx_test.T[0]\n",
    "\n",
    "X_train = [X[k] for k in idx_train]\n",
    "X_test = [X[k] for k in idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[str(k)]+[str(vary_sentence(k))] for k in X_train]\n",
    "import itertools\n",
    "X_train = list(itertools.chain(*X_train))\n",
    "y_train = np.column_stack((y_train, y_train)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prep for keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What an uninformed comment [ ' It ’ s exactly the opposite ' ]\n",
      "[37, 30, 3648, 352, 3, 10, 68, 143, 420, 1, 385, 3]\n"
     ]
    }
   ],
   "source": [
    "# changes words to numbers\n",
    "# tokenizing based on train data (i.e. will see new words in testset)\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=num_words, lower=True,\n",
    "                                                filters='!\"\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ”][',\n",
    "                                               split=\" \",oov_token=0)\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "#X2 = np.array(tokenizer.texts_to_sequences([X[k] for k in range(len(X))]))\n",
    "print(X_train[95])\n",
    "x_train = np.array(tokenizer.texts_to_sequences(X_train))\n",
    "x_test = np.array(tokenizer.texts_to_sequences(X_test))\n",
    "print(x_train[95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am so grateful for the chance to live even if my first few years weren't so great[' All people deserve at least a chance to make their own lives']\n",
      "[9, 119, 39, 3848, 15, 1, 494, 2, 225, 67, 21, 45, 235, 431, 301, 2295, 39, 493, 3, 52, 34, 694, 43, 277, 4, 494, 2, 94, 46, 239, 3849]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[910])\n",
    "print(x_train[910])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_index = {}\n",
    "# maxlen = 100\n",
    "# EMBEDDING_DIM = 100\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# import os\n",
    "# f = open(os.path.join('','glove.6B.100d.txt'))\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "\n",
    "\n",
    "# from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,CuDNNLSTM\n",
    "\n",
    "# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# embedding_layer = Embedding(len(word_index) + 1,\n",
    "#                             EMBEDDING_DIM,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=maxlen,\n",
    "#                             trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9760179540761441\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBr0kNc6gl6TGGfSS1DiDXpIa979iU1fINGMWwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x140fd3710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = [len(x) for x in x_train]\n",
    "_ = plt.hist(L,  bins=50,log=True)\n",
    "print(np.sum(np.array(L)<70) / len(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "161742 train sequences\n",
      "20219 test sequences\n",
      "Pad sequences (samples x time)\n",
      "removing len zero and len1 sentences...\n",
      "161742\n",
      "20219\n",
      "x_train shape: (161742, 70)\n",
      "x_test shape: (20219, 70)\n",
      "Train...\n",
      "Train on 161742 samples, validate on 20219 samples\n",
      "Epoch 1/50\n",
      "161742/161742 [==============================] - 427s 3ms/step - loss: 0.5961 - acc: 0.6692 - val_loss: 0.5751 - val_acc: 0.6948\n",
      "Epoch 2/50\n",
      "161742/161742 [==============================] - 411s 3ms/step - loss: 0.4500 - acc: 0.7770 - val_loss: 0.5643 - val_acc: 0.7101\n",
      "Epoch 3/50\n",
      "161742/161742 [==============================] - 436s 3ms/step - loss: 0.3548 - acc: 0.8264 - val_loss: 0.6103 - val_acc: 0.7229\n",
      "Epoch 4/50\n",
      "161742/161742 [==============================] - 1044s 6ms/step - loss: 0.2834 - acc: 0.8619 - val_loss: 0.6638 - val_acc: 0.7308\n",
      "Epoch 5/50\n",
      "161742/161742 [==============================] - 428s 3ms/step - loss: 0.2316 - acc: 0.8872 - val_loss: 0.6850 - val_acc: 0.7349\n",
      "Epoch 6/50\n",
      "161742/161742 [==============================] - 479s 3ms/step - loss: 0.1917 - acc: 0.9072 - val_loss: 0.7864 - val_acc: 0.7424\n",
      "Epoch 7/50\n",
      "161742/161742 [==============================] - 468s 3ms/step - loss: 0.1617 - acc: 0.9222 - val_loss: 0.8425 - val_acc: 0.7456\n",
      "Epoch 8/50\n",
      " 64000/161742 [==========>...................] - ETA: 4:37 - loss: 0.1313 - acc: 0.9378"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM,GRU, Bidirectional,CuDNNLSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "maxlen = 70\n",
    "print('Loading data...')\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print('removing len zero and len1 sentences...')\n",
    "\n",
    "nonzero1 = x_train.sum(1)>1\n",
    "x_train = x_train[nonzero1]\n",
    "y_train = y_train[nonzero1]\n",
    "\n",
    "print(len(x_train))\n",
    "nonzero2 = x_test.sum(1)>1\n",
    "x_test = x_test[nonzero2]\n",
    "y_test = y_test[nonzero2]\n",
    "print(len(x_test))\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 50, input_length=maxlen))\n",
    "model.add(Bidirectional(GRU(30)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=50,shuffle=True,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example= ['I believe women should be able to choose their own fate they should not be restrained']\n",
    "# prolife is zero prochoice is 1\n",
    "ex = tokenizer.texts_to_sequences(example)\n",
    "ex = np.array(sequence.pad_sequences(ex, maxlen=maxlen))\n",
    "print(ex.shape)\n",
    "print(model.predict(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ['abortion is wrong It should be made illegal']\n",
    "ex = tokenizer.texts_to_sequences(example)\n",
    "ex = np.array(sequence.pad_sequences(ex, maxlen=maxlen))\n",
    "print(model.predict(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('data_files/abortion/bilstm_syn2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('data_files/abortion/tokenizer_syn2.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # loading\n",
    "# with open('tokenizer.pickle', 'rb') as handle:\n",
    "#     tokenizer = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
